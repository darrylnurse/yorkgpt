
# Language Models

"Natural Language Processing (NLP) enables computers and digital devices to recognize, understand and generate text and speech by combining computational linguistics—the rule-based modeling of human language—together with statistical modeling, machine learning (ML) and deep learning." ([Jim Holdsworth, IBM, June 6th 2024](https://www.ibm.com/topics/natural-language-processing))

With NLP algorithms, developers are able to create Generative Artificial Intelligence (GenAI) language models which understand, contextualize and generate human-like text and speech.

## Large Language Models

Large Language Models (LLMs) are models trained on vast amounts of textual data from various sources. They can provide relational and semantic context between commonly occurring words, sentences and language structures. The textual data originates from existing websites, documents, books and articles, and serves as the scope of knowledge at which the LLM can operate. Vast sources of information allow them to emulate a broad spectrum of human language in various contexts.

## Small Language Models

Small Language Models (SLMs) are not trained on vast amounts of data, but rather precise information relating to its use case. While their ability to process language structures and semantics is less than that of LLMs, they benefit from superior efficiency and computational resourcefulness, as they are designed to operate within smaller, more defined scopes.

## Applications of LLMs and SLMs