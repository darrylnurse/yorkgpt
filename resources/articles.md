# Articles
- [The Illustrated Transformer Series](https://news.ycombinator.com/item?id=35712334)
- [Minimizing Hallucinations in LLMs](https://www.turing.com/resources/minimize-llm-hallucinations-strategy#what-are-llm-hallucinations?)
- [GPU vs TPU vs NPU](https://www.backblaze.com/blog/ai-101-gpu-vs-tpu-vs-npu/)
- [RAG From Scratch](https://pub.towardsai.net/rag-from-scratch-66c5eff02482)
- [Transformers Explained Visually](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452)
- [Finetune your own LLM Locally](https://medium.com/@thomasjvarghese49/run-your-own-fine-tuned-large-language-model-locally-without-any-internet-using-llama-cpp-part-1-a5db72801ccc)
- [Re-Training for Fine-tuned LLMs](https://www.linkedin.com/pulse/re-training-strategy-fine-tuned-llms-debmalya-biswas-o63te/)
- [Understanding Limitations of Vector-Based RAG](https://medium.com/thirdai-blog/understanding-the-fundamental-limitations-of-vector-based-retrieval-for-building-llm-powered-48bb7b5a57b3)